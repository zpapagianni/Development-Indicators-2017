---
title: "\\vspace{-2.5cm} Study Report "
subtitle: "EDAV Assessed Coursework 5\\ Zoi Papagianni (01827902)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H')
knitr::opts_chunk$set(fig.align = 'center')
library(dplyr)
library(irlba)
library(ggplot2)
library(ggpubr)
library(tidyverse)
library(reshape2)
library(caret)
library(Metrics)
library(gridExtra)
library(corrplot)
library(PerformanceAnalytics)
library(ggmap)
library(maps)
library(leaflet)
library(naniar)
library(RColorBrewer)
library(rgdal)
library(ggcorrplot)
library(factoextra)
library(scales)
library(ggrepel)
library(GGally)

#Set seed to replicate results
set.seed(9)
#Load data
dev.inc<- read_csv("data.csv") 
dev.inc<-dev.inc %>%select(-Education.Expend)
num.data<-dev.inc %>% 
  select_if(is.numeric)
#  Read this shape file with the rgdal library found
#http://thematicmapping.org/downloads/world_borders.php
my_spdf <- readOGR( dsn= "/Users/papagiannizoe/Desktop/MSc-Imperial/Term 2/Data visualisation/Assessments/Assessment_5/TM_WORLD_BORDERS_SIMPL-0.3" , 
                    layer="TM_WORLD_BORDERS_SIMPL-0.3", verbose=FALSE)

##Maps
#Prepare maps
mapWorld <-map('world',col="grey", fill=TRUE, bg="white", lwd=0.05, mar=rep(0,4),border=0, ylim=c(-80,80) )
```

# Clustering
The observations within our data are categorised by income and the region they belong to. The observations belonging to the same categories seem to present similar patterns and share a similar marginal distribution, apart from access to Electricity. To identify the possible set of subgroups, we’re going to reduce the dimension of our dataset to assist in finding any clustering behaviour that is less apparent in high dimensions. Moreover, we're going to test K-means clustering, which is another method often used for splitting a dataset into a set of n groups.

First, we're going to generate a two-dimensional visualization using generalised pairs plot for our data and density estimation using contour plots. We can't see any apparent subgroups from the output. 
```{r,Fig23,echo=FALSE,fig.align='center',fig.cap="\\label{fig:fig23} Income Categories"}
#Pairwise visualisation
num.data %>% select(-Access2Elec.pcnt,-Arable.Land.pcnt) %>%  
  ggpairs(progress = FALSE, # suppress verbose progress bar output
          lower = list(continuous = 'density'))
```
## Principal Compoment Analysis
As a first step, we standardize the data scale, implement PCA, and get the corresponding eigenvalues of each PC. If our data is well suited for PCA we should be able to discard these components while retaining at least 80% of the cumulative variance. The cumulative variance of the two first principals is equal to 61.4%, as shown in the results below.

```{r,echo=FALSE}
# Compute data with principal components ------------------
pca_devinc <- prcomp(num.data, center = TRUE, scale. = TRUE)  # compute principal components
#Create dataframe with the two first principal compoments 
df_pc<-data.frame(PC1=pca_devinc$x[,1],PC2=pca_devinc$x[,2],income=dev.inc$income,region=dev.inc$region)
#Cumulative proportion
cumsum_evalues <- cumsum(pca_devinc$sdev^2)
sum_evalues <- sum(pca_devinc$sdev^2)
#Print results
cat("Proportion of variance across top k PCs:",cumsum_evalues/sum_evalues,"\n")
```
Next we're going to visualise the results.The scree plot in the center shows that the first 4 components explain more than 0.1 of the variances. As a result, if we want to reduce dimensionality by losing only a small proportion of variance, we should choose the first 4 PCs, which account for 85.2% of the proportion of variance.
```{r,Fig24,echo=FALSE,fig.align='center',fig.cap="\\label{fig:fig24} Income Categories"}
#Proportion of variance explained by top k PCs plot
ggplot(data = data.frame(index=1:10,var_prop=(cumsum_evalues/sum_evalues)*100),
       hjust=0.9,vjust=0.2,size=3) +
  geom_point(mapping = aes(x=index,y=var_prop)) + 
  geom_line(mapping = aes(x=index,y=var_prop))+
  geom_text(mapping = aes(x=index,y=var_prop, label= round(var_prop,1)),hjust=0.5,vjust=1.5,size=3)+
  #Limits and scale
  scale_x_continuous(breaks = seq(1,10))+
  scale_y_continuous(limits=c(0,100),breaks = seq(0,100,10),labels= function(x) paste0(x, "%"))+
  #Theme customization
  theme_bw()+
  theme(legend.position = c(0.85, 0.9),
        legend.title = element_text(size = 10), 
        legend.text = element_text(size = 10),
        legend.key.size = unit(0.2, "cm"),
        plot.title = element_text(size = 10),
        text = element_text(size = 10),
        axis.line = element_line(colour = "black",size = 0.25))+
  #Axis and plot title
  ggtitle("PCA:Proportion of variance explained by top 10 PCs") +
  ylab("Proportion of variance") + 
  xlab("Principal components")

#Screeplot with eigenvalues
var_explained_df <- data.frame(PC= seq(1:10),
                               var_explained=(pca_devinc$sdev)^2/sum((pca_devinc$sdev)^2))

var_explained_df %>%
  ggplot(aes(x=PC,y=var_explained))+
  geom_point()+
  geom_line()+
  theme_bw()+
  theme(plot.title = element_text(size = 10),
        text = element_text(size = 10),
        axis.line = element_line(colour = "black",size = 0.25))+
  scale_x_continuous(breaks = 1:10)+
  ggtitle("Scree plot: PCA on scaled data")+
  xlab("Principal Component") + 
  ylab("Variance Explained")
```
The below graph depicts the projection in two dimensions and groupings based on the income category provided in the original dataset. The graph on the left is organized by region, and we can see there are no subgroups within the observations. The graph on the right is organized by income. We can see that the upper middle income and high-income categories are overlapping, and thus we can distinguish only two subgroups. We may need to use more principal components in order to achieve better clustering since the first two PCs account for a small portion of the variance.
```{r,Fig25,echo=FALSE,fig.align='center',fig.cap="\\label{fig:fig25} Income Categories"}
ggplot(df_pc, aes(PC1, PC2, col=region)) + 
  geom_point(size=2) +   # draw points
  coord_cartesian(xlim = 1.2 * c(min(df_pc$PC1), max(df_pc$PC1)), 
                  ylim = 1.2 * c(min(df_pc$PC2), max(df_pc$PC2))) +
  theme_bw()+
  theme(legend.position =c(0.8,0.8),
        legend.title = element_text(size = 7), 
        legend.text = element_text(size = 5),
        legend.key.size = unit(0.2, "cm"),
        plot.title = element_text(size = 9),
        text = element_text(size = 9),
        axis.line = element_line(colour = "black",size = 0.25))+
  scale_color_brewer(palette="Set1")+
  labs(title="Clustering behavior", 
       color = "Region")+
  xlab(paste0('PC1(',round(cumsum_evalues[2]*10,1), "%)"))+
  ylab(paste0('PC1(',round(cumsum_evalues[1]*10,1), "%)"))

ggplot(df_pc, aes(PC1, PC2, col=income)) + 
  geom_point(size=2) +   # draw points
  labs(title="Clustering behavior", 
       color = "Income category") + 
  coord_cartesian(xlim = 1.2 * c(min(df_pc$PC1), max(df_pc$PC1)), 
                  ylim = 1.2 * c(min(df_pc$PC2), max(df_pc$PC2))) +
  theme_bw()+
  theme(legend.position =c(0.75,0.8),
        legend.title = element_text(size = 7), 
        legend.text = element_text(size = 6),
        legend.key.size = unit(0.1, "cm"),
        plot.title = element_text(size = 9),
        text = element_text(size = 9),
        axis.line = element_line(colour = "black",size = 0.25))+
  scale_color_brewer(palette="Set1")+
  xlab(paste0('PC1(',round(cumsum_evalues[2]*10,1), "%)"))+
  ylab(paste0('PC1(',round(cumsum_evalues[1]*10,1), "%)"))

fviz_pca_ind(pca_devinc, geom.ind = "point", pointshape = 21, 
             pointsize = 2, 
             fill.ind = dev.inc$income, 
             addEllipses = TRUE,
             palette = 'Set1',
             label = "var",
             col.var = "black",
             repel = TRUE,
             legend.title = "Income Category") +
  ggtitle("Clustering behavior:2D PCA-plot") +
  theme_bw()+
  theme(legend.position ='top',
        legend.title = element_text(size = 7), 
        legend.text = element_text(size = 5),
        plot.title = element_text(size = 9),
        text = element_text(size = 9),
        axis.line = element_line(colour = "black",size = 0.25))
```
## K-means Clustering
Next we will perform k-means clustering on the original data set.We will calculate the total (minimised) within-cluster sum of squares (WCSS) for each value of k and silhouette stat charts to select the optimal value of k, which determines how well each observation fits within its cluster. The sihlouette method shows a clear peak for k=2 and the WSS shows a clear elbow at k=2 as well.
```{r,Fig26,echo=FALSE,fig.align='center',fig.cap="\\label{fig:fig26} Income Categories"}
#Best method
wss<-fviz_nbclust(num.data, kmeans, method = "wss")
sil<-fviz_nbclust(num.data, kmeans, method = "silhouette")
grid.arrange(wss,sil, nrow=2)
```
As a result we conclude that the optimum number of clusters is equal to 2 but out of interest we're going to visualize the results for 2,3 and for clusters. Tables 1–3 show which points from each category are assigned to which clusters.
```{r,Fig27,echo=FALSE,fig.align='center',fig.cap="\\label{fig:fig27} Income Categories"}
# plots to compare
dev.inc_k2 <- kmeans(num.data, centers = 2, nstart = 25)
dev.inc_k3 <- kmeans(num.data, centers = 3, nstart = 25)
dev.inc_k4 <- kmeans(num.data, centers = 4, nstart = 25)

p1<-fviz_cluster(dev.inc_k2, geom = "point",
             palette='Set1',
             data = num.data,
             labelsize = 0) + 
  ggtitle("Number of clusters : k = 2")+
  theme_bw()+
  theme(legend.position =c(0.9,0.15),
        legend.title = element_text(size = 7), 
        legend.text = element_text(size = 6),
        legend.key.size = unit(0.11, "cm"),
        plot.title = element_text(size = 9),
        text = element_text(size = 9),
        axis.line = element_line(colour = "black",size = 0.25))

p2 <- fviz_cluster(dev.inc_k3,
                   palette='Set1',
                   data = num.data,
                   labelsize = 0) + 
  ggtitle("Number of clusters : k = 3")+
  theme_bw()+
  theme(legend.position =c(0.9,0.15),
        legend.title = element_text(size = 7), 
        legend.text = element_text(size = 6),
        legend.key.size = unit(0.11, "cm"),
        plot.title = element_text(size = 9),
        text = element_text(size = 9),
        axis.line = element_line(colour = "black",size = 0.25))

p3 <- fviz_cluster(dev.inc_k4,    
                   palette='Set1',
                   data = num.data,
                   labelsize = 0) + 
  ggtitle("Number of clusters : k = 4")+
  theme_bw()+
  theme(legend.position =c(0.9,0.15),
        legend.title = element_text(size = 7), 
        legend.text = element_text(size = 6),
        legend.key.size = unit(0.11, "cm"),
        plot.title = element_text(size = 9),
        text = element_text(size = 9),
        axis.line = element_line(colour = "black",size = 0.25))

grid.arrange(p1, p2, p3, ncol = 3)
```
The tables below show which points from each category are assigned to which clusters.
```{r,echo=FALSE}
#compare results
knitr::kable(table(dev.inc_k2$cluster,dev.inc$income), "html")
knitr::kable(table(dev.inc_k3$cluster,dev.inc$income), "html")
knitr::kable(table(dev.inc_k4$cluster,dev.inc$income), "html")
```
As we can see when k=2, all points are in the first cluster apart from 18 from the high-income category. The same happens when k=3, where again some points from high income are being put in wrong clusters. Finally, we observe that even when k=4, the observations are not grouped correctly. We conclude that there are possibly two cluster in which we can group our observations which is not determined by the income category or region.